# Cryptocurrency Big Data Processing Pipeline

A comprehensive real-time cryptocurrency data processing system built with modern big data technologies including Hadoop, Spark, Kafka, and NiFi.

## ğŸ—ï¸ Project Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  CoinGecko API  â”‚â”€â”€â”€â–¶â”‚ Crypto       â”‚â”€â”€â”€â–¶â”‚     MySQL       â”‚â”€â”€â”€â–¶â”‚     NiFi        â”‚
â”‚                 â”‚    â”‚ Crawler      â”‚    â”‚   Database      â”‚    â”‚   Data Flow     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                                            â”‚
                                                                            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     Kafka       â”‚â—€â”€â”€â”€â”‚    Spark     â”‚â—€â”€â”€â”€â”‚      HDFS       â”‚â—€â”€â”€â”€â”‚     NiFi        â”‚
â”‚   Streaming     â”‚    â”‚  Processing  â”‚    â”‚   Storage       â”‚    â”‚   Output        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                       â”‚
         â–¼                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Spring Boot    â”‚    â”‚   Console    â”‚
â”‚   Consumer      â”‚    â”‚   Logging    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“‚ Project Structure

```
swd_gr_1/
â”œâ”€â”€ crypto/                      # Data ingestion layer
â”‚   â”œâ”€â”€ crypto_crawler.js        # CoinGecko API crawler
â”‚   â”œâ”€â”€ db.js                   # MySQL connection pool
â”‚   â”œâ”€â”€ db.sql                  # Database schema
â”‚   â”œâ”€â”€ package.json            # Node.js dependencies
â”‚   â””â”€â”€ README.md               # Crypto crawler documentation
â”œâ”€â”€ hadoop/                     # Big data infrastructure
â”‚   â”œâ”€â”€ docker-compose.yaml     # Hadoop ecosystem orchestration
â”‚   â”œâ”€â”€ docker-compose-kafka.yml # Kafka integration
â”‚   â”œâ”€â”€ hadoop.env              # Environment configuration
â”‚   â”œâ”€â”€ hadoop-conf/            # Hadoop configuration files
â”‚   â”œâ”€â”€ script/                 # Spark processing scripts
â”‚   â””â”€â”€ README.md               # Hadoop infrastructure docs
â”œâ”€â”€ crypto-spark-job/           # Real-time processing
â”‚   â”œâ”€â”€ app/main.py             # Spark streaming application
â”‚   â”œâ”€â”€ docker-compose.yml      # Spark job deployment
â”‚   â”œâ”€â”€ Dockerfile              # Container definition
â”‚   â”œâ”€â”€ requirements.txt        # Python dependencies
â”‚   â””â”€â”€ README.md               # Spark job documentation
â”œâ”€â”€ crypto-ui/                  # Frontend dashboard
â”‚   â”œâ”€â”€ src/                    # React TypeScript source code
â”‚   â”œâ”€â”€ components/             # UI components for data visualization
â”‚   â”œâ”€â”€ hooks/                  # Custom React hooks for data fetching
â”‚   â”œâ”€â”€ services/               # WebSocket and API services
â”‚   â”œâ”€â”€ package.json            # Node.js dependencies
â”‚   â””â”€â”€ README.md               # Frontend documentation
â””â”€â”€ kafka/                      # Stream consumption
    â”œâ”€â”€ src/                    # Spring Boot Kafka consumer
    â”œâ”€â”€ pom.xml                 # Maven dependencies
    â””â”€â”€ SwdApplication.java     # Main application class
```

## ğŸš€ System Components

### 1. Data Ingestion Layer (`crypto/`)

- **Technology**: Node.js + MySQL
- **Function**: Crawls cryptocurrency data from CoinGecko API
- **Features**:
  - Fetches top 250 cryptocurrencies every 5 minutes
  - Validates and parses essential data fields
  - Stores to MySQL with transaction safety
  - Graceful error handling and retry logic

### 2. Big Data Infrastructure (`hadoop/`)

- **Technology**: Hadoop HDFS + Apache NiFi + MySQL
- **Function**: Distributed storage and data flow management
- **Components**:
  - **NameNode**: HDFS metadata management (Port: 9870)
  - **DataNodes (3x)**: Distributed data storage
  - **ResourceManager**: YARN resource management (Port: 8088)
  - **NiFi**: ETL workflows and data integration (Port: 8443)
  - **MySQL**: Relational data storage (Port: 3306)

### 3. Real-time Processing (`crypto-spark-job/`)

- **Technology**: Apache Spark Streaming + Python
- **Function**: Real-time data aggregation and analytics
- **Features**:
  - Reads Avro data from HDFS
  - Calculates cryptocurrency averages by ID
  - 10-second processing triggers
  - Dual output: Kafka + Console logging
  - Fault-tolerant checkpointing

### 4. Stream Consumption (`kafka/`)

- **Technology**: Spring Boot + Apache Kafka
- **Function**: Consumes processed data streams
- **Features**:
  - Java 21 + Spring Boot 3.5.0
  - Kafka integration for real-time consumption
  - WebSocket support for real-time web interfaces

### 5. Frontend Dashboard (`crypto-ui/`)

- **Technology**: React + TypeScript + Vite
- **Function**: Real-time data visualization dashboard
- **Features**:
  - Real-time crypto data display via WebSocket
  - Interactive charts and tables
  - Dual data sources: Static JSON and live Kafka streams
  - Responsive design with Tailwind CSS
  - ShadCN UI component library integration

## ğŸ”„ Data Flow Pipeline

1. **Data Collection**:

   - Crypto crawler fetches market data from CoinGecko API
   - Data validation and formatting
   - Storage in MySQL database

2. **Data Integration**:

   - NiFi extracts data from MySQL
   - Transforms and loads into HDFS in Avro format
   - Provides data lineage and monitoring

3. **Stream Processing**:

   - Spark reads streaming Avro files from HDFS
   - Performs real-time aggregations by cryptocurrency
   - Calculates averages for price, market cap, volume, etc.

4. **Data Distribution**:
   - Processed results streamed to Kafka topic
   - Spring Boot consumer processes Kafka messages
   - Console logging for monitoring and debugging

## ğŸ› ï¸ Quick Start Guide

### Prerequisites

- Docker Engine 20.10+
- Docker Compose 1.29+
- Node.js 16+ (for crypto crawler)
- Java 21+ (for Kafka consumer)
- Minimum 8GB RAM
- Minimum 15GB free disk space

### 1. Start Hadoop Infrastructure

```bash
cd hadoop/
docker-compose up -d
```

### 2. Set up Crypto Crawler

```bash
cd crypto/
npm install
cp .env.example .env
docker-compose up -d
```

### 3. Deploy Spark Processing Job

```bash
cd crypto-spark-job/
docker-compose up -d
```

### 4. Start Kafka Consumer

```bash
cd kafka/
./mvnw spring-boot:run
```

### 5. Launch Frontend Dashboard

```bash
cd crypto-ui/
npm install --legacy-peer-deps
npm run dev
```

Access the dashboard at http://localhost:5173

## ğŸ“Š Monitoring & Access Points

### Web Interfaces

- **Hadoop NameNode**: http://localhost:9870
- **YARN ResourceManager**: http://localhost:8088
- **Apache NiFi**: https://localhost:8443 (admin/admin123)
- **Spark UI**: http://localhost:4040
- **Crypto Dashboard**: http://localhost:5173

### API Endpoints

- **Spring Boot App**: http://localhost:9090
- **MySQL Database**: localhost:3306

## ğŸ“ˆ Key Metrics & Analytics

The system processes and calculates:

- **Price Analytics**: Average current price, 24h high/low
- **Market Metrics**: Average market cap and ranking
- **Volume Analysis**: Average trading volume
- **Change Indicators**: 24h price and market cap changes
- **Real-time Aggregations**: Grouped by cryptocurrency ID

## ğŸ”§ Configuration

### Environment Variables

```bash
# Crypto Crawler (.env)
API_URL=https://api.coingecko.com/api/v3/coins/markets
API_KEY=your_coingecko_api_key
VS_CURRENCY=usd
COIN_COUNT=250
REQUEST_INTERVAL_MS=300000

# Hadoop (hadoop.env)
CORE_CONF_fs_defaultFS=hdfs://namenode:9000
YARN_CONF_yarn_resourcemanager_hostname=resourcemanager
```

### Data Schemas

- **Input**: Cryptocurrency market data (JSON from API)
- **Storage**: Normalized relational schema (MySQL)
- **Processing**: Avro format (HDFS)
- **Output**: Aggregated JSON (Kafka)

## ğŸš¨ Troubleshooting

### Common Issues

1. **Services not starting**: Check Docker resources and port conflicts
2. **Data not flowing**: Verify NiFi processors and HDFS connectivity
3. **Spark job failing**: Check HDFS paths and schema compatibility
4. **API rate limits**: Adjust crawler interval or upgrade API plan

### Debug Commands

```bash
# Check service status
docker-compose ps

# View logs
docker-compose logs [service_name]

# Access containers
docker exec -it [container_name] bash

# Check HDFS status
docker exec namenode hdfs dfsadmin -report
```

## ğŸ” Security Considerations

- **NiFi**: Basic authentication (change default credentials)
- **MySQL**: Secure root password in production
- **Network**: Internal Docker network isolation
- **API Keys**: Secure storage of CoinGecko credentials
- **Volumes**: Persistent data storage with proper permissions

## ğŸ“ Development & Contribution

### Adding New Features

1. **Data Sources**: Extend crypto crawler for additional APIs
2. **Processing Logic**: Modify Spark jobs for new analytics
3. **Storage**: Add new data models in MySQL schema
4. **Consumption**: Extend Spring Boot consumer for new use cases

### Best Practices

- Follow containerization patterns
- Implement proper error handling
- Add comprehensive logging
- Write unit tests for business logic
- Document configuration changes

## ğŸ“„ License

This project is part of SWD Group 1 coursework and is intended for educational purposes.

## ğŸ¤ Team & Support

**SWD Group 1** - Software Development Project

For issues and questions:

- Check component-specific README files
- Review Docker logs for troubleshooting
- Consult Apache Spark/Hadoop documentation
- Contact project maintainers

---

**Note**: This system is designed for educational big data processing scenarios. For production deployment, implement additional security, monitoring, and scaling considerations.
